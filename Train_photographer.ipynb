{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c831b5b-3025-4177-bef5-25aaec89573a",
   "metadata": {},
   "source": [
    "# **Running, monitoring and evaluating a training job**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9a96d-668e-40e2-bb63-a24b9213948d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### About this interactive guide\n",
    "\n",
    "This Jupyter Notebook is part of our [A Step-by-Step Guide for Non-Technical Folks on Training Stable Diffusion with a low-cost Cloud GPU](https://learn2train.medium.com/a-step-by-step-guide-for-non-technical-folks-on-training-stable-diffusion-with-a-low-cost-cloud-gpu-344c6b250d64). \n",
    "\n",
    "In this guide, we'll cover the following topics in an interactive way:\n",
    "\n",
    "1. **Fine-tuning a Stable Diffusion base model with a custom dataset**.\n",
    "      \n",
    "2. **Download the training dataset**. \n",
    "\n",
    "3. **Start the training job**\n",
    "    \n",
    "4. **Monitor your sample generations in Weights & Biases (W&B)**. W&B is a free tool used to visualise machine learning experiments. No installation is required as it will be run from a standalone webpage.\n",
    "\n",
    "5. **Training is done** \n",
    "\n",
    "6. **Upload the fine-tuned models to Hugging Face (optional)** so you can re-use them later. Hugging Face is an open-source community for AI experts and enthusiasts. It’s free to use!\n",
    "       \n",
    "7. **Evaluate the fine-tuned checkpoints** to asses its performance.\n",
    "\n",
    "8. **Terminate the GPU instance**. Avoid incurring charges by destroying the GPU instance. \n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "This interactive tutorial assumes you have:\n",
    "\n",
    "- Setup the training application on a cloud GPU platform according to [this guide](https://learn2train.medium.com/a-step-by-step-guide-for-non-technical-folks-on-training-stable-diffusion-with-a-low-cost-cloud-gpu-344c6b250d64)\n",
    "- A basic understanding of how Jupyter Notebooks work (if you don't check this [cool introduction to Jupyter Notebook demo](https://jupyter.org/try-jupyter/notebooks/?path=notebooks/Intro.ipynb)!)\n",
    "- A reliable internet connection.\n",
    "- An updated browser such as Chrome, Safari, Firefox, etc. \n",
    "- Time to train (it will take about 20 minutes to train the training dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef18e8-b5aa-41fd-8cc8-3ad07d19307f",
   "metadata": {},
   "source": [
    "# Download the training dataset\n",
    "\n",
    "### Download and extract the dataset \n",
    "\n",
    "We are going to download an already prepared training dataset into our GPU instance.\n",
    "\n",
    "A dataset is said to be prepared when every image has a caption describing it. It may or may not include other configuration settings read by the training application. \n",
    "\n",
    "Our image dataset contains 109 images, 109 text files, and 1 tag configuration file (`global.yaml`) that adds a suffix tag to each text file (in this case appends the phrase `in the style of Bella Kotak` to each caption description for each image). For more information about how to create a dataset please refer to chapter II of the tutorial.\n",
    "\n",
    "This is an example of how images and caption files are formatted in our dataset:\n",
    "\n",
    "* `image-name_001.jpg`\n",
    "* `image-name_001.txt`  <= Same filename as the jpg file\n",
    "\n",
    "The text file `image-name_001.txt` contains the caption describing `image-name_001.jpg`, say, for example: `a photo of a woman wearing a floral crown and holding a bouquet of flowers in the style of Bella Kotak`.\n",
    "\n",
    "\n",
    "\n",
    "Running the cell below will download a public ZIP file from Google Drive, extract it and store it in the **input** subfolder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d9f489-5a4d-4816-b66d-a84a2a00993c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /home/ekai8/.local/lib/python3.10/site-packages (from gdown) (3.17.0)\n",
      "Requirement already satisfied: requests[socks] in /home/ekai8/.local/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/ekai8/.local/lib/python3.10/site-packages (from gdown) (4.67.1)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/ekai8/.local/lib/python3.10/site-packages (from beautifulsoup4->gdown) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ekai8/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ekai8/miniconda3/envs/everydream/lib/python3.10/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ekai8/miniconda3/envs/everydream/lib/python3.10/site-packages (from requests[socks]->gdown) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ekai8/miniconda3/envs/everydream/lib/python3.10/site-packages (from requests[socks]->gdown) (2025.4.26)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 gdown-5.2.0 soupsieve-2.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Ifk07HeqxHfCCOCvb5oDF-cdxfkfsuq-\n",
      "To: /mnt/data2tb/xyz/EveryDream2trainer/input/dataset.zip\n",
      "100%|██████████████████████████████████████| 20.9M/20.9M [00:11<00:00, 1.83MB/s]\n",
      "\u001b[0m\u001b[01;34mdataset\u001b[0m/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Install gdown (to be able to download files from Google Drive)\n",
    "!pip install gdown\n",
    "\n",
    "# Download dataset\n",
    "os.makedirs('input', exist_ok=True)\n",
    "path_to_dataset = \"input/dataset.zip\"\n",
    "\n",
    "if not os.path.exists(path_to_dataset):\n",
    "    !gdown 1Ifk07HeqxHfCCOCvb5oDF-cdxfkfsuq- -O input/dataset.zip\n",
    "else:\n",
    "    print(f\"Already downloaded `{path_to_dataset}`\")\n",
    "\n",
    "# Unzip dataset into 'input' folder\n",
    "with zipfile.ZipFile(path_to_dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall('input/dataset')\n",
    "\n",
    "# Remove zip file\n",
    "os.remove(path_to_dataset)\n",
    "\n",
    "# List input directory\n",
    "%ls input/\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub \n",
    "\n",
    "path = kagglehub.get_path(\"downshift/russian-classic-painting-dataset\")\n",
    "print(\"path to dataset:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fcd56-0418-4be1-a5c3-38aa679b1aaf",
   "metadata": {},
   "source": [
    "# Start the training job\n",
    "\n",
    "Once our training images and their captions are inside the **input folder** we are ready to train the model. \n",
    "\n",
    "\n",
    "### Training configuration\n",
    "\n",
    "* **project name**: \"sd1_kotak\" <= Name of the project. It is convenient to name it in a way that identifies it from other training sessions.\n",
    "* **data_root**: \"input\" <= Folder location of the training images\n",
    "* **max epochs**: 60 <= An epoch refers to the one entire passing of training images through the trainer. We are doing 60 entire passes.  \n",
    "* **batch size**: 6 <= Determines the amount of images that are going to be trained every epoch\n",
    "* **sample steps**: 80 <= Determines how frequently samples are generated. In this case we will save every 20 epoch steps.   \n",
    "* **save every n epochs**: 20 <= Checkpoints will be saved every 20 epochs (since we are doing 60 epochs, we will end with 3 checkpoints) \n",
    "* **save ckpt dir**: \"ouput\" <= Folder location of the saved checkpoints\n",
    "* **zero_frequency_noise_ratio**: 0.04 <= This will make dark scenes more realistic  \n",
    "* **optimizer_config**: optimizer-photo.json <= We add an optimiser config file to get better results\n",
    "* **cond_dropout**: 0.0 <= This will prevent the trainer learning images without captions\n",
    "\n",
    "\n",
    "The are more configurations not show here. For a detailed explanation of each check [EveryDream 2 documentation](https://github.com/victorchall/EveryDream2trainer/blob/main/doc/TRAINING.md). \n",
    "\n",
    "### Download the optimizer configuration file\n",
    "\n",
    "Run the following cell to get the optimiser configuration settings to improve our training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f655d4fc-7975-4ca3-bb58-767de9177a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-10 09:11:08--  https://raw.githubusercontent.com/learn2train/l2t-sd/main/notebooks/optimizer-photo.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2039 (2,0K) [text/plain]\n",
      "Saving to: ‘optimizer-photo.json.1’\n",
      "\n",
      "optimizer-photo.jso 100%[===================>]   1,99K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-10 09:11:08 (19,6 MB/s) - ‘optimizer-photo.json.1’ saved [2039/2039]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/learn2train/l2t-sd/main/notebooks/optimizer-photo.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0141a5-f13f-46f6-a9a5-5bcf6190c0fc",
   "metadata": {},
   "source": [
    "### Set up Weights & Biases (W&B) for monitoring sample generation \n",
    "\n",
    "If you have a W&B account you can use it to track your training progress.  If you don't have one, you can create your W&B account for free at https://wandb.ai/site.\n",
    "\n",
    "You can get your API key from your [User Settings](https://wandb.ai/settings). Paste it in the following cell where it says \"PUT-YOUR-API-KEY-HERE\", keep the double quotes, and then RUN the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f74cc3-d90f-465e-9aaa-0468b83df1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb_token = \"c4e12fd9221dd0f6200288a38e3d5a94a0993ed5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54fbcf-b95e-415c-ad8a-78494ff728bb",
   "metadata": {},
   "source": [
    "The cell above should look like:\n",
    "    \n",
    "`wandb_token = \"28d37291d39f337237291d39f391d39f3\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0a39e-8204-4017-98ef-4ae2451511be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Running the training session\n",
    "\n",
    "To start training run the cell below. The cell will start printing its log. Keep scrolling down to monitor the current status of the training session. \n",
    "\n",
    "**IMPORTANT: If you see messages with a red backround, IGNORE THEM as they are only warning messages** \n",
    "\n",
    "The training takes about 20 minutes on a RTX 3090 with 24GB of VRAM. \n",
    "\n",
    "While you wait for the `Training completed` message, watch the samples being generated in Weights & Biases (see cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f73fb86-ebef-41e2-9382-4aa11be84be6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/root/.netrc': Permission denied\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using legacy-service, which is deprecated. If this is unintentional, you can fix it by ensuring you do not call `wandb.require('legacy-service')` and do not set the WANDB_X_REQUIRE_LEGACY_SERVICE environment variable.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ekai8/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "** Your branch 'main' is up to date with the remote\n",
      "Loading training config from None.\n",
      "Error on loading training config from None.\n",
      " logging to logs/sd1_kotak-20250510-091115/sd1_kotak-20250510-091115.log\n",
      " Args:\n",
      "{'amp': True,\n",
      " 'aspects': [[512, 512],\n",
      "             [576, 448],\n",
      "             [448, 576],\n",
      "             [640, 384],\n",
      "             [384, 640],\n",
      "             [768, 320],\n",
      "             [320, 768],\n",
      "             [896, 256],\n",
      "             [256, 896],\n",
      "             [1024, 256],\n",
      "             [256, 1024]],\n",
      " 'attn_type': 'sdp',\n",
      " 'batch_size': 1,\n",
      " 'ckpt_every_n_minutes': 1000000000.0,\n",
      " 'clip_grad_norm': None,\n",
      " 'clip_skip': 0,\n",
      " 'cond_dropout': 0.0,\n",
      " 'config': None,\n",
      " 'data_root': 'input',\n",
      " 'disable_amp': False,\n",
      " 'disable_textenc_training': False,\n",
      " 'disable_unet_training': False,\n",
      " 'ema_decay_rate': None,\n",
      " 'ema_device': 'cpu',\n",
      " 'ema_resume_model': None,\n",
      " 'ema_sample_ema_model': False,\n",
      " 'ema_sample_nonema_model': False,\n",
      " 'ema_strength_target': None,\n",
      " 'ema_update_interval': 500,\n",
      " 'embedding_perturbation': 0.0,\n",
      " 'enable_zero_terminal_snr': None,\n",
      " 'flip_p': 0.0,\n",
      " 'gpuid': 0,\n",
      " 'grad_accum': 1,\n",
      " 'gradient_checkpointing': False,\n",
      " 'keep_tags': 0,\n",
      " 'load_settings_every_epoch': None,\n",
      " 'log_step': 25,\n",
      " 'logdir': 'logs',\n",
      " 'loss_type': 'mse_huber',\n",
      " 'lr': None,\n",
      " 'lr_decay_steps': 0,\n",
      " 'lr_scheduler': 'constant',\n",
      " 'lr_warmup_steps': None,\n",
      " 'max_epochs': 60,\n",
      " 'min_snr_gamma': None,\n",
      " 'no_prepend_last': False,\n",
      " 'no_save_ckpt': False,\n",
      " 'optimizer_config': 'optimizer-photo.json',\n",
      " 'plugins': None,\n",
      " 'project_name': 'sd1_kotak',\n",
      " 'pyramid_noise_discount': None,\n",
      " 'rated_dataset': False,\n",
      " 'rated_dataset_target_dropout_percent': 50,\n",
      " 'resolution': 512,\n",
      " 'resume_ckpt': 'learn2train/stable-diffusion-v1-5',\n",
      " 'run_name': None,\n",
      " 'sample_prompts': 'sample_prompts.txt',\n",
      " 'sample_steps': 80,\n",
      " 'save_ckpt_dir': 'output',\n",
      " 'save_ckpts_from_n_epochs': 0,\n",
      " 'save_every_n_epochs': 20,\n",
      " 'save_full_precision': False,\n",
      " 'save_optimizer': False,\n",
      " 'seed': 555,\n",
      " 'shuffle_tags': False,\n",
      " 'timestep_end': 1000,\n",
      " 'timestep_start': 0,\n",
      " 'train_sampler': 'ddpm',\n",
      " 'validation_config': None,\n",
      " 'wandb': True,\n",
      " 'write_schedule': False,\n",
      " 'zero_frequency_noise_ratio': 0.04}\n",
      " Seed: 555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekai8/miniconda3/envs/everydream/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "text_encoder/model.safetensors not found\n",
      "text_encoder/model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fe20b0d89f4a8fb1551353dc3c0f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " unet attention_head_dim: 8\n",
      "Inferred yaml: v1-inference.yaml, attn: sd1, prediction_type: epsilon\n",
      "* HuggingFace Downloaded model from learn2train/stable-diffusion-v1-5 to /home/ekai8/.cache/huggingface/hub/models--learn2train--stable-diffusion-v1-5/snapshots/c65eab801b6c8d3407a1c6f89569d4465fc1f43f.\n",
      "** Using attention yaml file: v1-inference.yaml, is_sd1_attn: True.\n",
      " * Using default (DDPM) noise scheduler for training: ddpm\n",
      "* Using SDP attention *\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mi-ngocthanh1\u001b[0m (\u001b[33mi-ngocthanh1-ekmap\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/data2tb/xyz/EveryDream2trainer/wandb/run-20250510_091126-rgeznqo3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/i-ngocthanh1-ekmap/sd1_kotak/runs/rgeznqo3' target=\"_blank\">devout-planet-4</a></strong> to <a href='https://wandb.ai/i-ngocthanh1-ekmap/sd1_kotak' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/i-ngocthanh1-ekmap/sd1_kotak' target=\"_blank\">https://wandb.ai/i-ngocthanh1-ekmap/sd1_kotak</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/i-ngocthanh1-ekmap/sd1_kotak/runs/rgeznqo3' target=\"_blank\">https://wandb.ai/i-ngocthanh1-ekmap/sd1_kotak/runs/rgeznqo3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* DLMA resolution 512, buckets: [[512, 512], [576, 448], [448, 576], [640, 384], [384, 640], [768, 320], [320, 768], [896, 256], [256, 896], [1024, 256], [256, 1024]]\n",
      " Preloading images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preloading: 100%|██████████| 109/109 [00:01<00:00, 93.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Found 109 files in 'input'\n",
      "No plugins specified\n",
      " * DLMA initialized with 109 images.\n",
      " ** Dataset 'train': 109 batches, num_images: 109, batch_size: 1\n",
      "\n",
      " raw optimizer_config:\n",
      "{'apply_grad_scaler_step_tweaks': True,\n",
      " 'base': {'betas': [0.9, 0.999],\n",
      "          'epsilon': 1e-08,\n",
      "          'lr': 1e-06,\n",
      "          'lr_decay_steps': None,\n",
      "          'lr_scheduler': 'constant',\n",
      "          'lr_warmup_steps': None,\n",
      "          'optimizer': 'adamw8bit',\n",
      "          'weight_decay': 0.01},\n",
      " 'text_encoder_freezing': {'unfreeze_last_n_layers': 6},\n",
      " 'text_encoder_overrides': {'betas': None,\n",
      "                            'epsilon': None,\n",
      "                            'lr': 5e-07,\n",
      "                            'lr_decay_steps': None,\n",
      "                            'lr_scheduler': 'cosine',\n",
      "                            'lr_warmup_steps': None,\n",
      "                            'optimizer': None,\n",
      "                            'weight_decay': None}}\n",
      " Final unet optimizer config:\n",
      "{'betas': [0.9, 0.999],\n",
      " 'epsilon': 1e-08,\n",
      " 'lr': 1e-06,\n",
      " 'lr_decay_steps': 9810,\n",
      " 'lr_scheduler': 'constant',\n",
      " 'lr_warmup_steps': 196,\n",
      " 'optimizer': 'adamw8bit',\n",
      " 'weight_decay': 0.01}\n",
      " Final text encoder optimizer config:\n",
      "{'betas': [0.9, 0.999],\n",
      " 'epsilon': 1e-08,\n",
      " 'lr': 5e-07,\n",
      " 'lr_decay_steps': 9810,\n",
      " 'lr_scheduler': 'cosine',\n",
      " 'lr_warmup_steps': 196,\n",
      " 'optimizer': 'adamw8bit',\n",
      " 'weight_decay': 0.01}\n",
      " ❄️ freezing embeddings\n",
      " ❄️ freezing text encoder layers 1-6 out of 12 layers total\n",
      "\u001b[94m text encoder weight normal: 305.7\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m unet weight normal: 1174.3\u001b[0m\n",
      "\u001b[36m * text encoder optimizer: AdamW8bit (98 parameters) *\u001b[0m\n",
      "\u001b[36m    lr: 5e-07, betas: [0.9, 0.999], epsilon: 1e-08, weight_decay: 0.01 *\u001b[0m\n",
      "\u001b[36m * unet optimizer: AdamW8bit (686 parameters) *\u001b[0m\n",
      "\u001b[36m    lr: 1e-06, betas: [0.9, 0.999], epsilon: 1e-08, weight_decay: 0.01 *\u001b[0m\n",
      " Grad scaler enabled: True (amp mode)\n",
      " * SampleGenerator initialized with 1 prompts, generating samples every 80 training steps, using scheduler 'ddim' with 30 inference steps\n",
      " \u001b[92m** Welcome to EveryDream trainer 2.0!**\u001b[0m\n",
      " (C) 2022-2023 Victor C Hall  This program is licensed under AGPL 3.0 https://www.gnu.org/licenses/agpl-3.0.en.html\n",
      "\n",
      "** Trainer Starting **\n",
      " Pretraining GPU Memory: 4890 / 12288 MB\n",
      " saving ckpts every 1000000000.0 minutes\n",
      " saving ckpts every 20 epochs\n",
      " unet device: cuda:0, precision: torch.float32, training: True\n",
      " text_encoder device: cuda:0, precision: torch.float32, training: True\n",
      " vae device: cuda:0, precision: torch.float16, training: False\n",
      " scheduler: <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>\n",
      " \u001b[32mProject name: \u001b[0m\u001b[92msd1_kotak\u001b[0m\n",
      " \u001b[32mgrad_accum: \u001b[0m\u001b[92m1\u001b[0m\n",
      " \u001b[32mbatch_size: \u001b[0m\u001b[92m1\u001b[0m\n",
      " \u001b[32mepoch_len: \u001b[92m109\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dd41267b98499eb81754ff4b1ec116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e833c0d2e3414057b1dc89116db16d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data2tb/xyz/EveryDream2trainer/train.py:1013: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  early_timestep_bias = torch.tensor(early_timestep_bias, dtype=torch.float).to(unet.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:79 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7e53c4da274c8b99821d2030dfac6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:159 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d88cf946e8a43ed9933fa20979a331f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:239 for 1 prompts\n",
      " * Generating samples at gs:319 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00130962bcf7460ca6dd22cef5f0ba75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:399 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019936b9a89746339e93481a696164e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:479 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500e62f612774f6ba1c5f08c8bd211f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:559 for 1 prompts\n",
      " * Generating samples at gs:639 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dbbd772385421eb60fef4441d98a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:719 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0113d6eb9734109a787572682ec21fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:799 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec926def75348ad8ba1c2c4120977ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:879 for 1 prompts\n",
      " * Generating samples at gs:959 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85523f8ad8f44ef080c978f36141210e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:1039 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab65031eb024abca6ce2c5507cbdad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:1119 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f11be16e7840ecbb3cd73915fca846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:1199 for 1 prompts\n",
      " * Generating samples at gs:1279 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d466d915b1249f4950df80e0b930d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:1359 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75766168618c44208d21bb31c489eb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:1439 for 1 prompts\n",
      " * Generating samples at gs:1519 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7b9324ba9341b09079911ab3621c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:1599 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2512e2e7b8114d31a883e9daaf0298fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:1679 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c6301d41b747f595b9687224973e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:1759 for 1 prompts\n",
      " * Generating samples at gs:1839 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edffeefe4344fdeb13434634c1a6442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:1919 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3594607cf9a4b0ab589f0a8f518cabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:1999 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc812711ad34432b896070186c81173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:2079 for 1 prompts\n",
      " * Generating samples at gs:2159 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1996025d9b1c4a3693afc950a2c69977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saving model, 20 epochs at step 2180\n",
      " * Saving diffusers model to logs/sd1_kotak-20250510-091115/ckpts/sd1_kotak-ep20-gs02180\n",
      " * Saving SD model to output/sd1_kotak-ep20-gs02180.safetensors\n",
      " * Generating samples at gs:2239 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f82c162f2843838aad7d5252e102ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:2319 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fe55c1d9c24ca2a3cca768e549b956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:2399 for 1 prompts\n",
      " * Generating samples at gs:2479 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9fe6362f6542de8345eddbaa999e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:2559 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87178697fca340ffb979bc4622898776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:2639 for 1 prompts\n",
      " * Generating samples at gs:2719 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8dbf666c564f1d9f01fb2b024d5918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:2799 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae844d5f70c4aa2902d9dcba694c435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:2879 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4461d41142384e59a461b9f1513330f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:2959 for 1 prompts\n",
      " * Generating samples at gs:3039 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69416e764a734397abb7c721e6420d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:3119 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3946064ae942b5bd0f75b60275ba3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:3199 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a629d223bc564a46852543bcf5659973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:3279 for 1 prompts\n",
      " * Generating samples at gs:3359 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8067538df8a848b38e9eb644973488fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:3439 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1174892ac4e4cfeb78de8dfd8e12180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:3519 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340d31fb186f4c0986dae728852f6752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:3599 for 1 prompts\n",
      " * Generating samples at gs:3679 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ded2d7b2f934baf84e720edc6176a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:3759 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4e7d268e654888921ecfd1a569f848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:3839 for 1 prompts\n",
      " * Generating samples at gs:3919 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365a563407374853a725c75649c0b0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:3999 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa80a9bb9bf04628a18b4cbc3cd98d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:4079 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01c09e79a44431a9e8526d83ac6143a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:4159 for 1 prompts\n",
      " * Generating samples at gs:4239 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecaaa49cb134431e995802baae150c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:4319 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b89051b8234717bcbe840945c9a6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saving model, 20 epochs at step 4360\n",
      " * Saving diffusers model to logs/sd1_kotak-20250510-091115/ckpts/sd1_kotak-ep40-gs04360\n",
      " * Saving SD model to output/sd1_kotak-ep40-gs04360.safetensors\n",
      " * Generating samples at gs:4399 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e7ecb058974c9e9be924b67d2508a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:4479 for 1 prompts\n",
      " * Generating samples at gs:4559 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e97bff1cf894479a1c2bc029fcd5b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:4639 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43ec57bbd6f4015a64a0623dddbbb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:4719 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b53d79aa3124277b8a29ee04e8d2e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:4799 for 1 prompts\n",
      " * Generating samples at gs:4879 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9613e05709c4865be72d0488b1cb1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:4959 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4926548f802b43e4b2e4e384b60927f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:5039 for 1 prompts\n",
      " * Generating samples at gs:5119 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9035ad05273a45648229cf4a3fae610b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:5199 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e416089ed0614c159e6bf57e67e184de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:5279 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e991103c98141c5936c8ec692789312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:5359 for 1 prompts\n",
      " * Generating samples at gs:5439 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686969549a3b41b78f6d53f46bef1df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:5519 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e17462498f74299ab502143949ed2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:5599 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310934740a0b4c2f82fc0cc269b5e987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:5679 for 1 prompts\n",
      " * Generating samples at gs:5759 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13faa862ab3646a7b8a5523b2f3595bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:5839 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb1e282813e41dfb7562be61a65a146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:5919 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0416264090cd46739e0248832a9fbff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:5999 for 1 prompts\n",
      " * Generating samples at gs:6079 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c83392bdae46a8bc986d832e42cb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:6159 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d17cab9be842e68896252ea61f00c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:6239 for 1 prompts\n",
      " * Generating samples at gs:6319 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0366a387c05d48b3a8e449b17e272c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:6399 for 1 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccfcbc09016423bb13ce8f24faed592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:6479 for 1 prompts\n",
      " * Saving diffusers model to logs/sd1_kotak-20250510-091115/ckpts/last-sd1_kotak-ep60-gs06540\n",
      " * Saving SD model to output/last-sd1_kotak-ep60-gs06540.safetensors\n",
      "\u001b[36mTraining complete\u001b[0m\n",
      "Total training time took 72.41 minutes, total steps: 6540\n",
      "Average epoch time: 1.20 minutes\n",
      "\u001b[97m ***************************\u001b[0m\n",
      "\u001b[97m **** Finished training ****\u001b[0m\n",
      "\u001b[97m ***************************\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get the wandb token\n",
    "wandb_settings = \"\"\n",
    "if wandb_token:\n",
    "  !rm /root/.netrc\n",
    "  !wandb login $wandb_token\n",
    "  wandb_settings = \"--wandb\"\n",
    "\n",
    "# Start the training\n",
    "\n",
    "%run train.py --resume_ckpt \"learn2train/stable-diffusion-v1-5\" \\\n",
    "$wandb_settings \\\n",
    "--project_name \"sd1_kotak\" \\\n",
    "--data_root \"input\" \\\n",
    "--max_epochs 60 \\\n",
    "--sample_steps 80 \\\n",
    "--batch_size 1 \\\n",
    "--save_every_n_epochs 20 \\\n",
    "--zero_frequency_noise_ratio 0.04 \\\n",
    "--cond_dropout 0.0 \\\n",
    "--optimizer_config optimizer-photo.json \\\n",
    "--save_ckpt_dir \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0be597-abb7-4f52-b29e-11f18c387edb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Watch your samples in Weights & Biases while training is running\n",
    "\n",
    "### W&B dashboard\n",
    "\n",
    "Go to the [W&B dashboard](https://wandb.ai/home) in another browser tab. You will see your training run in your home page. \n",
    "\n",
    "Click on your training run to check the samples being generated. They should give you an idea how good/bad your model learning progress is going. You should stop the training once you are satisfied with the results you are seeing.\n",
    "\n",
    "Samples come in three. That is because each sample generated uses different CFG values (1, 4 and 7).\n",
    "\n",
    "![W&B](https://drive.google.com/uc?export=view&id=1G1fmv5uFN_pk57jBhmD7SVes4at-uv4C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd913f-3c0d-4014-b6e1-6b2fd9cfc2b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Training is finished\n",
    "\n",
    "Once the training is done you should see the following messages:\n",
    "\n",
    "![Training is finished](https://drive.google.com/uc?export=view&id=1WXwNcHaKStpuusvReueriEJXsl3rLWRM)\n",
    "\n",
    "\n",
    "That was it! The base model has been updated and you are now left with checkpoints.\n",
    "\n",
    "Before terminating the GPU instance, you could save the checkpoints to your computer, upload them to your Hugging Face repository, or transfer them to another storage plaform such as AWS S3.\n",
    "\n",
    "\n",
    "I strongly recommend that you **DO NOT** download the checkpoints to your computer because of the time it could take to save them. It's always better and faster to transfer them to AWS S3, Cloudflare R2, or to your Hugging Face as you'll see next.  \n",
    "\n",
    "If you insist in saving them to your computer you can use the file explorer on the left panel, double click in the **output** folder, select the checkpoints you want to download and click download file.  \n",
    "\n",
    "![output folder](https://drive.google.com/uc?export=view&id=1owrfdiPrJy7M0M7wkI9c5N_rBM8ukbx5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0982c-6029-4fc1-b161-f71c18680261",
   "metadata": {},
   "source": [
    "# 6. Upload your checkpoints to Hugging Face (optional but highly recommended)\n",
    "\n",
    "If you aren't saving your checkpoints to your computer, you could save them to your Hugging Face repository instead. That way you can easily re-use or share them. \n",
    "\n",
    "### Get a Hugging Face token\n",
    "\n",
    "If you haven't got one yet, have a look at [How to Host Stable Diffusion Checkpoints on Hugging Face for Free](https://learn2train.medium.com/a-step-by-step-guide-to-host-stable-diffusion-checkpoints-on-hugging-face-for-free-2098d0c18a01)\n",
    "\n",
    "### Log-in into your account \n",
    "\n",
    "Run the cell below and paste your **Hugging Face write token** into the prompt that will pop-up to log into your account (no need to check the **git credentials** box). You need to login to Hugging Face to be able to upload data into your repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b4a52-97ab-4d70-8db7-43d1203135fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face\n",
    "\n",
    "from huggingface_hub import notebook_login, hf_hub_download\n",
    "import os\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24eee3d-f5df-45f3-9acc-ee0206cfe6b1",
   "metadata": {},
   "source": [
    "### Upload checkpoints to your model repository\n",
    "\n",
    "Make sure you are **logged in** to Hugging Face running the above login cell first.\n",
    "\n",
    "Use the cell below to upload one or more checkpoints to your personal Hugging Face repository. You should already be authorized to interact with Hugging Face if you ran the cell above.\n",
    "\n",
    "When you run the cell below, a box will show up and you need to  **CLICK** to select which `.safetensors` file are marked for upload. This allows you to select which ones to upload.  If you don't click of the ckpts, nothing will happen.\n",
    "\n",
    "You will also be required to fill-in your username and your repository name:\n",
    "* Hugging Face username: **your username** (look in [HuggingFace account page](https://huggingface.co/settings/account)).\n",
    "* Hugging Face repository name: **your repo name**\n",
    "\n",
    "**WARNING**\n",
    "\n",
    "**If your Hugging Face account is brand new upload only 3 checkpoint files**. For safety reasons, Hugging Face limits the amount of files a new user can make. If you try to upload more than 3 checkpoint files you'll probably get a warning tell you to wait 24 hours to keep uploading. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df5e1a-3c68-41c0-a4ed-ea0abcd19858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after reading the instructions of the cell above. \n",
    "\n",
    "import glob\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from ipywidgets import *\n",
    "\n",
    "all_ckpts = [f for f in glob.glob(\"output/*.safetensors\")]\n",
    "  \n",
    "ckpt_picker = SelectMultiple(options=all_ckpts, layout=Layout(width=\"600px\")) \n",
    "hfuser = Text(placeholder='Hugging Face username')\n",
    "hfrepo = Text(placeholder='Hugging Face repository name')\n",
    "\n",
    "api = HfApi()\n",
    "upload_btn = Button(description='Upload')\n",
    "out = Output()\n",
    "\n",
    "def upload_ckpts(_):\n",
    "    repo_id=f\"{hfuser.value or hfuser.placeholder}/{hfrepo.value or hfrepo.placeholder}\"\n",
    "    with out:\n",
    "        if ckpt_picker is None or len(ckpt_picker.value) < 1:\n",
    "            print(\"Nothing selected for upload, make sure to click one of the ckpt files in the list, or, you have no ckpt files in the current directory.\")\n",
    "        for ckpt in ckpt_picker.value:\n",
    "            print(f\"Uploading to HF: huggingface.co/{repo_id}/{ckpt}\")\n",
    "            response = api.upload_file(\n",
    "                path_or_fileobj=ckpt,\n",
    "                path_in_repo=ckpt,\n",
    "                repo_id=repo_id,\n",
    "                repo_type=None,\n",
    "                create_pr=1,\n",
    "            )\n",
    "            display(response)\n",
    "        print(\"DONE\")\n",
    "\n",
    "upload_btn.on_click(upload_ckpts)\n",
    "box = VBox([ckpt_picker, HBox([hfuser, hfrepo]), upload_btn, out])\n",
    "\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4a28a-2488-4216-b431-fcc3e9d1de9e",
   "metadata": {},
   "source": [
    "### Save the uploads to your model repository\n",
    "\n",
    "To actually save the uploaded checkpoints to your repo, go back to your Hugging Face model repository and click the **Community** tab. You'll see a list of one or more checkpoints. Go one by one and click **Merge** to save them to your model repository:\n",
    "\n",
    "![Merge](https://drive.google.com/uc?export=view&id=1zyOcOq9uABW1dO69pNYenvsag1C7asyc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a00d16-9b84-492f-8e6a-defe71e82b43",
   "metadata": {},
   "source": [
    "# 7. Evaluate your fine-tuned checkpoints\n",
    "\n",
    "\n",
    "### Test inference on your checkpoints\n",
    "\n",
    "To recap: Training is over and you are left with model checkpoints (safetensor files). These checkpoints are updated fine-tuned models saved at different times during the training session. \n",
    "\n",
    "The main idea here is to evaluate each of your checkpoints to find the ones that generate the output you like the most.  \n",
    "\n",
    "Run the following cell to display a mini text-to-image generator. You can choose any checkpoint -or all of them- and set inference parameters such as **prompt, steps, CFG, resolution and seed**.\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb1a8cd-6a04-44e5-a770-c23ee247ce82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46ff5dcc4214dd4b77957acc48e55ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Checkpoint:', layout=Layout(width='600px'), options=('./logs/sd1_kotak-20…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import *\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, DDIMScheduler, DDPMScheduler, PNDMScheduler, EulerAncestralDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "\n",
    "checkpoints_ts = []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if os.path.basename(file) == \"model_index.json\":\n",
    "                ts = os.path.getmtime(os.path.join(root,file))\n",
    "                ckpt = root\n",
    "                checkpoints_ts.append((ts, root))\n",
    "\n",
    "checkpoints = [ckpt for (_, ckpt) in sorted(checkpoints_ts, reverse=True)]\n",
    "full_width = Layout(width='600px')\n",
    "half_width = Layout(width='300px')\n",
    "\n",
    "checkpoint = Dropdown(options=checkpoints, description='Checkpoint:', layout=full_width)\n",
    "prompt = Textarea(value='a photo of ', description='Prompt:', layout=full_width)\n",
    "height = IntSlider(value=512, min=256, max=768, step=32, description='Height:', layout=half_width)\n",
    "width = IntSlider(value=512, min=256, max=768, step=32, description='Width:', layout=half_width)\n",
    "cfg = FloatSlider(value=7.0, min=0.0, max=14.0, step=0.2, description='CFG Scale:', layout=half_width)\n",
    "steps = IntSlider(value=30, min=10, max=100, description='Steps:', layout=half_width)\n",
    "seed = IntText(value=-1, description='Seed:', layout=half_width)\n",
    "generate_btn = Button(description='Generate', layout=full_width)\n",
    "out = Output()\n",
    "\n",
    "def generate(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        display(f\"Loading model {checkpoint.value}\")\n",
    "        actual_seed = seed.value if seed.value != -1 else random.randint(0, 2**30)\n",
    "\n",
    "        text_encoder = CLIPTextModel.from_pretrained(checkpoint.value, subfolder=\"text_encoder\")\n",
    "        vae = AutoencoderKL.from_pretrained(checkpoint.value, subfolder=\"vae\")\n",
    "        unet = UNet2DConditionModel.from_pretrained(checkpoint.value, subfolder=\"unet\")\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(checkpoint.value, subfolder=\"tokenizer\", use_fast=False)\n",
    "        scheduler = DDIMScheduler.from_pretrained(checkpoint.value, subfolder=\"scheduler\")\n",
    "        text_encoder.eval()\n",
    "        vae.eval()\n",
    "        unet.eval()\n",
    "\n",
    "        text_encoder.to(\"cuda\")\n",
    "        vae.to(\"cuda\")\n",
    "        unet.to(\"cuda\")\n",
    "\n",
    "        pipe = StableDiffusionPipeline(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=None, # save vram\n",
    "            requires_safety_checker=None, # avoid nag\n",
    "            feature_extractor=None, # must be none of no safety checker\n",
    "        )\n",
    "\n",
    "        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "        \n",
    "        print(inspect.cleandoc(f\"\"\"\n",
    "              Prompt: {prompt.value}\n",
    "              Resolution: {width.value}x{height.value}\n",
    "              CFG: {cfg.value}\n",
    "              Steps: {steps.value}\n",
    "              Seed: {actual_seed}\n",
    "              \"\"\"))\n",
    "        with autocast(\"cuda\"):\n",
    "            image = pipe(prompt.value, \n",
    "                generator=torch.Generator(\"cuda\").manual_seed(actual_seed),\n",
    "                num_inference_steps=steps.value, \n",
    "                guidance_scale=cfg.value,\n",
    "                width=width.value,\n",
    "                height=height.value\n",
    "            ).images[0]\n",
    "        del pipe\n",
    "        gc.collect()\n",
    "        with torch.cuda.device(\"cuda\"):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        display(image)\n",
    "            \n",
    "generate_btn.on_click(generate)\n",
    "box = VBox(\n",
    "    children=[\n",
    "        checkpoint, prompt, \n",
    "        HBox([VBox([width, height]), VBox([steps, cfg])]), \n",
    "        seed, \n",
    "        generate_btn, \n",
    "        out]\n",
    ")\n",
    "\n",
    "\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa733fb-d112-4d2f-9771-5fc8ac11e0c8",
   "metadata": {},
   "source": [
    "# 8. Terminate your GPU instance when you are done\n",
    "\n",
    "Don't forget to terminate your cloud GPU instance once you are done evaluating your checkpoints, otherwise you will be still charged. Check the last section of the previous chapter to see how to terminate your instance. \n",
    "\n",
    "Note that once you terminate your instance **Jupyter Lab** will stop working. If you want to use it again you'll have to start a new training session on the same or difference GPU instance, and start all over again. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everydream",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
